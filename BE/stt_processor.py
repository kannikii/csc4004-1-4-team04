import os
import json
from pathlib import Path
from typing import Optional, List, Dict, Any

os.environ.setdefault("KMP_DUPLICATE_LIB_OK", "TRUE")

import torch
import whisper
from moviepy.editor import VideoFileClip
from firebase_admin import credentials, firestore
import firebase_admin
from dotenv import load_dotenv
from openai import OpenAI

try:
    from faster_whisper import WhisperModel as FasterWhisperModel
except ImportError:  # pragma: no cover - optional dep
    FasterWhisperModel = None

load_dotenv()

# ------------------------------------
# ğŸ“Œ í™˜ê²½ ë³€ìˆ˜ ê¸°ë°˜ ì„¤ì •ê°’
# ------------------------------------
BASE_DIR = Path(__file__).resolve().parent
DEFAULT_CRED_PATH = BASE_DIR / "serviceAccountKey.json"

FIREBASE_PROJECT_ID = os.getenv("FIREBASE_PROJECT_ID")
FIREBASE_USER_ID = os.getenv("FIREBASE_USER_ID", "default_user")
INPUT_VIDEO_DIR = Path(os.getenv("STT_INPUT_VIDEO_DIR", BASE_DIR / "videos"))
OUTPUT_AUDIO_DIR = Path(os.getenv("STT_OUTPUT_AUDIO_DIR", BASE_DIR / "results/audio_wav"))
OUTPUT_JSON_DIR = Path(os.getenv("STT_OUTPUT_JSON_DIR", BASE_DIR / "results/stt_json"))
CREDENTIAL_PATH = Path(os.getenv("FIREBASE_CRED_PATH", DEFAULT_CRED_PATH))
WHISPER_MODEL_SIZE = os.getenv("WHISPER_MODEL_SIZE", "base")  # 'base', 'small', 'medium' ë“± ì„ íƒ
WHISPER_VERBOSE = os.getenv("WHISPER_VERBOSE", "false").lower() in {"1", "true", "yes", "on"}
STT_ENGINE = os.getenv("STT_ENGINE", "faster").lower()
if STT_ENGINE not in {"faster", "openai"}:
    STT_ENGINE = "faster"
WHISPER_DEVICE = os.getenv("WHISPER_DEVICE", "auto").lower()
FASTER_WHISPER_COMPUTE_TYPE = os.getenv("FASTER_WHISPER_COMPUTE_TYPE", "int8")
PAUSE_THRESHOLD_SEC = float(os.getenv("PAUSE_THRESHOLD_SEC", "2.0"))

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL")  # ê¸°ë³¸ê°’(None) ì‹œ OpenAI ê³µì‹ ì—”ë“œí¬ì¸íŠ¸
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# (ì˜µì…˜) ì¶”í›„ OpenRouterë¡œ ì „í™˜í•  ë•Œë¥¼ ìœ„í•œ ì„¤ì •ê°’
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
OPENROUTER_BASE_URL = os.getenv("OPENROUTER_BASE_URL", "https://openrouter.ai/api/v1")
OPENROUTER_MODEL = os.getenv("OPENROUTER_MODEL", "openai/gpt-4o-mini")
OPENROUTER_SITE = os.getenv("OPENROUTER_SITE_URL", "")
OPENROUTER_TITLE = os.getenv("OPENROUTER_TITLE", "stt-processor")

HESITATION_PATTERNS = ["~í–ˆëŠ”ë°", "~ê°™ì•„ìš”", "~ë§ì´ì£ ", "~ë¼ë“ ì§€", "~ì…ë‹ˆë‹¤ë§Œ", "ì•½ê°„", "ì™ ì§€"]
FILLER_WORDS = ["ìŒ", "ì–´", "ì•„", "ì €", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­", "ì‚¬ì‹¤"]
HESITATION_LIST = ", ".join(HESITATION_PATTERNS)
FILLER_LIST = ", ".join(FILLER_WORDS)

_WHISPER_MODEL = None
_FASTER_WHISPER_MODEL = None
_stt_progress = {"progress": 0, "stage": "idle"}
_stt_last_logged = {"progress": -1, "stage": ""}
_firestore_client: Optional[firestore.Client] = None

_llm_client: Optional[OpenAI] = None
_llm_model: str = OPENAI_MODEL
_llm_headers: Dict[str, str] = {}
_llm_provider: str = "openai"

if OPENAI_API_KEY:
    _llm_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL or None)
    _llm_model = OPENAI_MODEL
    _llm_provider = "openai"
elif OPENROUTER_API_KEY:
    # í•„ìš” ì‹œ OpenRouterë¡œ ìŠ¤ìœ„ì¹­í•  ìˆ˜ ìˆë„ë¡ ë‚¨ê²¨ë‘” ë¶„ê¸°
    if OPENROUTER_SITE:
        _llm_headers["HTTP-Referer"] = OPENROUTER_SITE
    if OPENROUTER_TITLE:
        _llm_headers["X-Title"] = OPENROUTER_TITLE
    _llm_client = OpenAI(base_url=OPENROUTER_BASE_URL, api_key=OPENROUTER_API_KEY)
    _llm_model = OPENROUTER_MODEL
    _llm_provider = "openrouter"


def _clamp(value: int) -> int:
    return max(0, min(100, value))


def _resolve_device() -> str:
    if WHISPER_DEVICE != "auto":
        return WHISPER_DEVICE
    if torch.cuda.is_available():
        return "cuda"
    if hasattr(torch.backends, "mps") and torch.backends.mps.is_available():
        return "mps"
    return "cpu"


def set_stt_progress(progress: Optional[int] = None, stage: Optional[str] = None):
    global _stt_last_logged
    if progress is not None:
        _stt_progress["progress"] = _clamp(progress)
    if stage:
        _stt_progress["stage"] = stage

    if (
        _stt_progress["progress"] != _stt_last_logged["progress"]
        or _stt_progress["stage"] != _stt_last_logged["stage"]
    ):
        print(f"[STT] { _stt_progress['progress']:>3}% - {_stt_progress['stage']}")
        _stt_last_logged = dict(_stt_progress)


def get_stt_progress():
    return dict(_stt_progress)


def reset_stt_progress():
    set_stt_progress(0, "idle")


# ------------------------------------
# 1. Firebase ì´ˆê¸°í™” ë° DB í•¨ìˆ˜
# ------------------------------------
def initialize_firebase() -> bool:
    """Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        if firebase_admin._apps:
            return True

        if not CREDENTIAL_PATH.exists():
            print(f"âš ï¸ Firebase ì„œë¹„ìŠ¤ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {CREDENTIAL_PATH}")
            return False

        cred = credentials.Certificate(str(CREDENTIAL_PATH))
        options = {"projectId": FIREBASE_PROJECT_ID} if FIREBASE_PROJECT_ID else None
        firebase_admin.initialize_app(cred, options)
        print("âœ… Firebase Admin SDK ì´ˆê¸°í™” ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"âŒ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: ì˜¤ë¥˜: {e}")
        return False


def get_firestore_client():
    """ì´ˆê¸°í™”ëœ Firestore í´ë¼ì´ì–¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    global _firestore_client
    if _firestore_client is not None:
        return _firestore_client
    ok = initialize_firebase()
    if not ok:
        return None
    _firestore_client = firestore.client()
    return _firestore_client


def _get_presentation_doc(user_id: str, file_name: str):
    client = get_firestore_client()
    if not client:
        return None
    return (
        client.collection("users")
        .document(user_id)
        .collection("presentations")
        .document(file_name)
    )


def upload_to_firebase_text(user_id: str, file_name: str, stt_data: dict):
    """STT ì „ì‚¬ ê²°ê³¼ ì¤‘ 'full_text'ë§Œ DBì˜ stt_raw ê²½ë¡œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    doc_ref = _get_presentation_doc(user_id, file_name)
    if doc_ref is None:
        print("    -> [DB] Firestore í´ë¼ì´ì–¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    payload = {
        "stt_raw": {
            "full_text": stt_data.get("full_text"),
            "timestamps": stt_data.get("words"),
        },
        "stt_analysis": stt_data,
    }
    try:
        doc_ref.set(payload, merge=True)
        print("    -> [DB] STT ê²°ê³¼ ì—…ë¡œë“œ ì™„ë£Œ (Firestore).")
    except Exception as e:
        print(f"    -> [DB] Firestore ì—…ë¡œë“œ ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")


def upload_to_firebase_voice_analysis(user_id: str, file_name: str, analysis_data: dict):
    """ì–¸ì–´ ìŠµê´€ ë° WPM ë¶„ì„ ê²°ê³¼ë¥¼ DB voice_analysis ê²½ë¡œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    doc_ref = _get_presentation_doc(user_id, file_name)
    if doc_ref is None:
        print("    -> [DB] Firestore í´ë¼ì´ì–¸íŠ¸ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í•´ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    try:
        doc_ref.set({"voice_analysis": analysis_data}, merge=True)
        print("    -> [DB] voice_analysis ì—…ë¡œë“œ ì™„ë£Œ (Firestore).")
    except Exception as e:
        print(f"    -> [DB] voice_analysis ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")


# ------------------------------------
# 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ í•¨ìˆ˜
# ------------------------------------
def extract_audio(video_path: Path, output_audio_path: Path) -> bool:
    try:
        with VideoFileClip(str(video_path)) as video_clip:
            audio_clip = video_clip.audio
            audio_clip.write_audiofile(
                str(output_audio_path),
                codec='pcm_s16le',
                fps=16000,
                verbose=False,
                logger=None
            )
        return True
    except Exception as e:
        print(f"  âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return False


# ------------------------------------
# 3. Whisper STT ì „ì‚¬ ë° ë¶„ì„ ìë£Œ ìƒì„± í•¨ìˆ˜
# ------------------------------------
def get_whisper_model():
    global _WHISPER_MODEL
    if _WHISPER_MODEL is None:
        print(f"  -> [STT] Whisper {WHISPER_MODEL_SIZE} ëª¨ë¸ ë¡œë”© ì¤‘...")
        _WHISPER_MODEL = whisper.load_model(WHISPER_MODEL_SIZE)
    return _WHISPER_MODEL


def get_faster_whisper_model():
    global _FASTER_WHISPER_MODEL
    if FasterWhisperModel is None:
        raise RuntimeError("faster-whisper íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. pip install faster-whisper")
    if _FASTER_WHISPER_MODEL is None:
        device = _resolve_device()
        if device == "mps":
            print("âš ï¸ faster-whisperëŠ” MPSë¥¼ ì§€ì›í•˜ì§€ ì•Šì•„ CPUë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤. (.envì—ì„œ WHISPER_DEVICE=cpu ì§€ì • ê°€ëŠ¥)")
            device = "cpu"
        print(f"  -> [STT] faster-whisper {WHISPER_MODEL_SIZE} ëª¨ë¸ ë¡œë”© ì¤‘... (device={device}, compute={FASTER_WHISPER_COMPUTE_TYPE})")
        _FASTER_WHISPER_MODEL = FasterWhisperModel(
            WHISPER_MODEL_SIZE,
            device=device,
            compute_type=FASTER_WHISPER_COMPUTE_TYPE,
        )
    return _FASTER_WHISPER_MODEL


def transcribe_with_openai(audio_path: Path):
    print(f"  -> [STT] Whisper {WHISPER_MODEL_SIZE} (openai) ëª¨ë¸ ë¡œë”© ë° ì „ì‚¬ ì¤‘...")
    try:
        model = get_whisper_model()
        set_stt_progress(50, "Whisper ì¶”ë¡  ì¤‘")
        result = model.transcribe(
            str(audio_path),
            language="ko",
            word_timestamps=True,
            verbose=WHISPER_VERBOSE
        )

        full_text = result.get('text', '').strip()
        word_timestamps = []
        duration_sec = 0.0

        for segment in result.get('segments', []):
            if 'words' in segment:
                word_timestamps.extend(segment['words'])

        if word_timestamps:
            duration_sec = word_timestamps[-1].get('end', 0.0)

        analysis_data = {
            "full_text": full_text,
            "words": word_timestamps,
            "duration_sec": duration_sec,
            "word_count": len(word_timestamps)
        }

        print("  âœ… STT ì „ì‚¬ ì™„ë£Œ.")
        set_stt_progress(65, "STT ê²°ê³¼ ì •ë¦¬")
        return analysis_data

    except Exception as e:
        print(f"  âŒ Whisper ì „ì‚¬ ì‹¤íŒ¨: {e}")
        set_stt_progress(50, "Whisper ì˜¤ë¥˜")
        return None


def transcribe_with_faster(audio_path: Path):
    try:
        model = get_faster_whisper_model()
        set_stt_progress(45, "faster-whisper ì¶”ë¡  ì¤€ë¹„")
        segments, info = model.transcribe(
            str(audio_path),
            language="ko",
            beam_size=5,
            word_timestamps=True
        )
        collected_segments: List[Any] = list(segments)
        set_stt_progress(55, "faster-whisper ì¶”ë¡  ì¤‘")

        full_text = " ".join(seg.text.strip() for seg in collected_segments).strip()
        word_timestamps: List[Dict[str, Any]] = []
        for seg in collected_segments:
            if seg.words:
                for word in seg.words:
                    word_timestamps.append({
                        "word": word.word.strip(),
                        "start": float(word.start) if word.start is not None else None,
                        "end": float(word.end) if word.end is not None else None,
                        "probability": float(getattr(word, "probability", 0.0))
                    })

        duration_sec = float(info.duration) if info and info.duration else 0.0
        if not duration_sec and word_timestamps:
            duration_sec = float(word_timestamps[-1].get("end") or 0.0)

        analysis_data = {
            "full_text": full_text,
            "words": word_timestamps,
            "duration_sec": duration_sec,
            "word_count": len(word_timestamps)
        }

        set_stt_progress(65, "STT ê²°ê³¼ ì •ë¦¬")
        return analysis_data
    except Exception as e:
        print(f"  âŒ faster-whisper ì „ì‚¬ ì‹¤íŒ¨: {e}")
        set_stt_progress(50, "Whisper ì˜¤ë¥˜")
        return None


def whisper_transcribe(audio_path: Path):
    if STT_ENGINE == "openai":
        return transcribe_with_openai(audio_path)
    result = transcribe_with_faster(audio_path)
    if result is None:
        print("âš ï¸ faster-whisper ì‹¤íŒ¨, ê¸°ë³¸ Whisperë¡œ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
        return transcribe_with_openai(audio_path)
    return result


# ------------------------------------
# 4. GPT ê¸°ë°˜ ì–¸ì–´ ìŠµê´€ ë¶„ì„
# ------------------------------------
def analyze_speech_patterns_with_gpt(full_text: str) -> Dict[str, Any]:
    """LLM(ê¸°ë³¸: OpenAI, ì˜µì…˜: OpenRouter)ë¡œ ë§ë íë¦¼Â·ì¶”ì„ìƒˆë¥¼ JSONìœ¼ë¡œ ë°˜í™˜."""
    if not full_text:
        return {}
    if not _llm_client:
        print("âš ï¸ OPENAI_API_KEY/OPENROUTER_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•„ GPT ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {}

    system_prompt = (
        "ë‹¹ì‹ ì€ ë°œí‘œìì˜ ì–¸ì–´ ìŠµê´€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ 'ë§ë íë¦¼'ê³¼ 'ì¶”ì„ìƒˆ'ë¥¼ íƒì§€í•˜ê³ , "
        "JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”. íƒì§€ í›„, íƒì§€ëœ ëª¨ë“  ìš”ì†Œë¥¼ ì œê±°í•œ ì •ì œ í…ìŠ¤íŠ¸ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”. "
        f"íƒì§€ ê¸°ì¤€: ë§ë íë¦¼ ({HESITATION_LIST}), ì¶”ì„ìƒˆ ({FILLER_LIST})."
    )

    try:
        completion = _llm_client.chat.completions.create(
            model=_llm_model,
            response_format={"type": "json_object"},
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": full_text},
            ],
            extra_headers=_llm_headers or None,
        )
        return json.loads(completion.choices[0].message.content)
    except Exception as e:
        print(f"âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨({_llm_provider}): {e}")
        return {}


def analyze_voice_rhythm_and_patterns(stt_result_data: dict) -> dict:
    """WPM/ë¬´ìŒ/ì¶”ì„ìƒˆÂ·ë§ë ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    words = stt_result_data.get('words', [])
    total_duration = stt_result_data.get('duration_sec', 0.0)
    word_count = len(words)
    wpm = round((word_count / total_duration) * 60) if total_duration > 0 else 0

    pause_events: List[Dict] = []
    all_pause_durations: List[float] = []

    for i in range(len(words) - 1):
        current_word_end = words[i].get('end', 0.0)
        next_word_start = words[i+1].get('start', 0.0)
        gap_duration = next_word_start - current_word_end
        if gap_duration > 0:
            all_pause_durations.append(gap_duration)
        if gap_duration >= PAUSE_THRESHOLD_SEC:
            pause_events.append({
                "start_sec": round(current_word_end, 2),
                "end_sec": round(next_word_start, 2),
                "duration": round(gap_duration, 2)
            })

    total_pause_count = len(all_pause_durations)
    avg_pause_duration = round(sum(all_pause_durations) / total_pause_count, 2) if total_pause_count > 0 else 0.0
    long_pause_count = len(pause_events)
    full_text = stt_result_data.get('full_text', '')

    speech_patterns_result = analyze_speech_patterns_with_gpt(full_text)

    # GPT ë¶„ì„ ì‹¤íŒ¨ ì‹œ ë˜ëŠ” 0ì¼ ë•Œ Regex ê¸°ë°˜ ë°±ì—… ì¹´ìš´íŒ…
    hesitation_count = speech_patterns_result.get('hesitation_count', 0)
    filler_count = speech_patterns_result.get('filler_count', 0)
    
    if hesitation_count == 0:
        import re
        # HESITATION_PATTERNS = ["~í–ˆëŠ”ë°", "~ê°™ì•„ìš”", "~ë§ì´ì£ ", "~ë¼ë“ ì§€", "~ì…ë‹ˆë‹¤ë§Œ", "ì•½ê°„", "ì™ ì§€"]
        for pat in HESITATION_PATTERNS:
            # ë‹¨ìˆœ í¬í•¨ ì—¬ë¶€ ì²´í¬ (ì •í™•ë„ë¥¼ ìœ„í•´ì„  í˜•íƒœì†Œ ë¶„ì„ì´ í•„ìš”í•˜ë‚˜ ì—¬ê¸°ì„  ê°„ë‹¨íˆ)
            hesitation_count += len(re.findall(pat, full_text))

    if filler_count == 0:
        import re
        # FILLER_WORDS = ["ìŒ", "ì–´", "ì•„", "ì €", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­", "ì‚¬ì‹¤"]
        for word in FILLER_WORDS:
            # ë‹¨ì–´ ë‹¨ìœ„ ë§¤ì¹­ì„ ìœ„í•´ \b ì‚¬ìš© ê³ ë ¤, í•œêµ­ì–´ëŠ” ì¡°ì‚¬ ë•Œë¬¸ì— \bê°€ ì• ë§¤í•  ìˆ˜ ìˆìŒ.
            # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ì¹´ìš´íŒ…
            filler_count += len(re.findall(word, full_text))

    return {
        "raw_text_for_gpt": full_text,
        "wpm": wpm,
        "pause_events": pause_events,
        "avg_pause_duration": avg_pause_duration,
        "long_pause_count": long_pause_count,
        "hesitation_count": hesitation_count,
        "filler_count": filler_count,
        "hesitation_list": speech_patterns_result.get('hesitation_list', []),
        "filler_list": speech_patterns_result.get('filler_list', []),
        "text_for_logic_analysis": speech_patterns_result.get('text_for_logic_analysis', full_text),
    }


# ------------------------------------
# 5. í†µí•© ë°°ì¹˜/ë‹¨ì¼ ì²˜ë¦¬ í•¨ìˆ˜
# ------------------------------------
def process_single_video(
    video_path: Path,
    user_id: Optional[str] = None,
    output_audio_dir: Optional[Path] = None,
    output_json_dir: Optional[Path] = None,
    upload_to_firebase: bool = True,
    output_basename: Optional[str] = None,
    enable_gpt_analysis: bool = True,
):
    """ë‹¨ì¼ ì˜ìƒ íŒŒì¼ì— ëŒ€í•œ STT ë¶„ì„ ë° ê²°ê³¼ ì €ì¥."""
    set_stt_progress(0, "íŒŒì¼ ê²€ì¦")
    video_path = Path(video_path)
    if not video_path.exists():
        set_stt_progress(0, "íŒŒì¼ ì—†ìŒ")
        raise FileNotFoundError(f"ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")

    user_id = user_id or FIREBASE_USER_ID
    output_audio_dir = Path(output_audio_dir or OUTPUT_AUDIO_DIR)
    output_json_dir = Path(output_json_dir or OUTPUT_JSON_DIR)

    output_audio_dir.mkdir(parents=True, exist_ok=True)
    output_json_dir.mkdir(parents=True, exist_ok=True)

    base_name = output_basename or video_path.stem
    audio_path = output_audio_dir / f"{base_name}.wav"
    txt_path = output_json_dir / f"{base_name}_text.txt"
    json_path = output_json_dir / f"{base_name}_analysis.json"

    set_stt_progress(5, "ì˜¤ë””ì˜¤ ì¶”ì¶œ")
    if not extract_audio(video_path, audio_path):
        set_stt_progress(5, "ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨")
        raise RuntimeError("ì˜¤ë””ì˜¤ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    set_stt_progress(30, "Whisper ë¡œë”©")
    stt_result = whisper_transcribe(audio_path)
    if not stt_result:
        set_stt_progress(30, "STT ì‹¤íŒ¨")
        raise RuntimeError("STT ì „ì‚¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    set_stt_progress(70, "ê²°ê³¼ ì €ì¥")
    try:
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(stt_result['full_text'])
        print(f"  âœ… í…ìŠ¤íŠ¸ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {txt_path}")
    except Exception as e:
        print(f"  âŒ TXT íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    try:
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(stt_result, f, ensure_ascii=False, indent=4)
        print(f"  âœ… ë¶„ì„ ìë£Œ JSON ì €ì¥ ì™„ë£Œ: {json_path}")
    except Exception as e:
        print(f"  âŒ JSON íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    stt_result["file_paths"] = {
        "audio": str(audio_path),
        "text": str(txt_path),
        "json": str(json_path),
    }
    stt_result["base_name"] = base_name

    voice_analysis = None
    if enable_gpt_analysis and _llm_client:
        set_stt_progress(80, "GPT ì–¸ì–´ìŠµê´€ ë¶„ì„")
        voice_analysis = analyze_voice_rhythm_and_patterns(stt_result)
        stt_result["voice_analysis"] = voice_analysis

    if upload_to_firebase:
        set_stt_progress(85, "Firebase ì—…ë¡œë“œ ì¤€ë¹„")
        is_firebase_ok = initialize_firebase()
        if is_firebase_ok:
            upload_to_firebase_text(user_id, base_name, stt_result)
            if voice_analysis:
                upload_to_firebase_voice_analysis(user_id, base_name, voice_analysis)
        else:
            print("  âš ï¸ Firebase ì„¤ì •ì´ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    set_stt_progress(100, "ì™„ë£Œ")
    return stt_result


def process_multiple_videos(input_dir, output_dir_audio, output_dir_json, user_id):
    input_dir = Path(input_dir)
    video_files = [f for f in input_dir.glob("*.mp4")]

    if not video_files:
        print(f"ê²½ê³ : '{input_dir}'ì—ì„œ ì²˜ë¦¬í•  MP4 ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ì´ {len(video_files)}ê°œì˜ ì˜ìƒì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‚¬ìš©ì ID: {user_id}")

    for i, video_file in enumerate(video_files, start=1):
        print(f"\n--- [{i}/{len(video_files)}] {video_file.name} ì²˜ë¦¬ ì‹œì‘ ---")
        try:
            process_single_video(
                video_file,
                user_id=user_id,
                output_audio_dir=output_dir_audio,
                output_json_dir=output_dir_json,
            )
        except Exception as exc:
            print(f"  âŒ {video_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {exc}")


if __name__ == "__main__":
    process_multiple_videos(INPUT_VIDEO_DIR, OUTPUT_AUDIO_DIR, OUTPUT_JSON_DIR, FIREBASE_USER_ID)
