import os
import json
import whisper
from moviepy.editor import VideoFileClip
from firebase_admin import credentials, db
import firebase_admin
from openai import OpenAI
from typing import Optional, Dict, Any, List
import warnings
warnings.filterwarnings("ignore")

# -----------------------------------------------------------------
# ğŸ“Œ í”„ë¡œì íŠ¸ë³„ ì„¤ì •ê°’ (ìˆ˜ì • í•„ìˆ˜)
# -----------------------------------------------------------------
PROJECT_NAME = "P-CSC4004-C1-T4"
API_KEY = "sk-..." 
FIREBASE_DATABASE_URL = "https://csc4004-1-4-team04-default-rtdb.firebaseio.com/"
USER_ID = "2021111985_JungHyeon"
CREDENTIAL_PATH = "/content/drive/MyDrive/AI_Coach_Data/Firebase_Keys/csc4004-1-4-team04-adminsdk.json"
INPUT_VIDEO_DIR = "/content/drive/MyDrive/AI_Coach_Data/videos"
OUTPUT_AUDIO_DIR = "/content/drive/MyDrive/AI_Coach_Data/results/audio_wav"
OUTPUT_JSON_DIR = "/content/drive/MyDrive/AI_Coach_Data/results/stt_json"
WHISPER_MODEL_SIZE = "small"
PAUSE_THRESHOLD_SEC = 2.0

# ğŸ“Œ GPT ë¶„ì„ ê¸°ì¤€ ëª©ë¡
HESITATION_PATTERNS = ["~í–ˆëŠ”ë°", "~ê°™ì•„ìš”", "~ë§ì´ì£ ", "~ë¼ë“ ì§€", "~ì…ë‹ˆë‹¤ë§Œ", "ì•½ê°„", "ì™ ì§€"]
FILLER_WORDS = ["ìŒ", "ì–´", "ì•„", "ì €", "ê·¸ë‹ˆê¹Œ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ë­", "ì‚¬ì‹¤"]
HESITATION_LIST = ", ".join(HESITATION_PATTERNS)
FILLER_LIST = ", ".join(FILLER_WORDS)

# ğŸ“Œ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    client = OpenAI(api_key= API_KEY)
except Exception as e:
    print(f"âŒ OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ë¬¸ì œ: {e}")

# -----------------------------------------------------------------
# 1. Firebase ë° ë³´ì¡° í•¨ìˆ˜ ì •ì˜
# -----------------------------------------------------------------
def initialize_firebase() -> bool:
    """Firebase Admin SDKë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    try:
        if not firebase_admin._apps:
            cred = credentials.Certificate(CREDENTIAL_PATH)
            firebase_admin.initialize_app(cred, {'databaseURL': FIREBASE_DATABASE_URL})
        print("âœ… Firebase Admin SDK ì´ˆê¸°í™” ì™„ë£Œ.")
        return True
    except Exception as e:
        print(f"âŒ Firebase ì´ˆê¸°í™” ì‹¤íŒ¨: ì˜¤ë¥˜: {e}")
        return False

def upload_to_firebase_analysis(user_id: str, file_name: str, analysis_result: Dict) -> None:
    """WPM ë¶„ì„ ê²°ê³¼ë§Œ Firebaseì˜ voice_analysis ê²½ë¡œì— ì—…ë¡œë“œí•©ë‹ˆë‹¤."""
    ref_path_analysis = f'users/{user_id}/presentations/{file_name}/voice_analysis'
    try:
        analysis_data_to_save = analysis_result.copy()
        analysis_data_to_save.pop('raw_text_for_gpt', None)
        analysis_data_to_save.pop('text_for_logic_analysis', None)

        db.reference(ref_path_analysis).set(analysis_data_to_save)
        print(f"    -> [DB] WPM/ì¶”ì„ìƒˆ ë¶„ì„ ê²°ê³¼ ì—…ë¡œë“œ ì™„ë£Œ.")

    except Exception as e:
        print(f"    -> [DB] Firebase ì—…ë¡œë“œ ì‹¤íŒ¨. ì˜¤ë¥˜: {e}")

def extract_audio(video_path: str, output_audio_path: str) -> bool:
    """ì˜ìƒ íŒŒì¼ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•˜ì—¬ WAV íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with VideoFileClip(video_path) as video_clip:
            audio_clip = video_clip.audio
            audio_clip.write_audiofile(
                output_audio_path,
                codec='pcm_s16le',
                fps=16000,
                verbose=False,
                logger=None
            )
        return True
    except Exception as e:
        print(f"  âŒ ì˜¤ë””ì˜¤ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return False

def whisper_transcribe(audio_path: str) -> Optional[Dict]:
    """Whisperë¥¼ ì‚¬ìš©í•˜ì—¬ STT ì „ì‚¬ ë° ë‹¨ì–´ë³„ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print(f"  -> [STT] Whisper {WHISPER_MODEL_SIZE} ëª¨ë¸ ë¡œë”© ë° ì „ì‚¬ ì¤‘...")
    try:
        model = whisper.load_model(WHISPER_MODEL_SIZE)
        result = model.transcribe(audio_path, language="ko", word_timestamps=True)

        full_text = result.get('text', '').strip()
        word_timestamps = []
        for segment in result.get('segments', []):
            if 'words' in segment:
                word_timestamps.extend(segment['words'])

        duration_sec = word_timestamps[-1].get('end', 0.0) if word_timestamps else 0.0

        return {
            "full_text": full_text, "words": word_timestamps,
            "duration_sec": duration_sec, "word_count": len(word_timestamps)
        }

    except Exception as e:
        print(f"  âŒ Whisper ì „ì‚¬ ì‹¤íŒ¨: {e}")
        return None

# -----------------------------------------------------------------
# 2. GPT ê¸°ë°˜ ì–¸ì–´ ìŠµê´€ ë¶„ì„ ë¡œì§ 
# -----------------------------------------------------------------
def analyze_speech_patterns_with_gpt(full_text: str) -> Optional[Dict[str, Any]]:
    """GPT APIë¥¼ í˜¸ì¶œí•˜ì—¬ ë§ë íë¦¼ê³¼ ì¶”ì„ìƒˆë¥¼ íƒì§€í•˜ê³  ì •ì œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    if not full_text: return {}

    system_prompt = (
        "ë‹¹ì‹ ì€ ë°œí‘œìì˜ ì–¸ì–´ ìŠµê´€ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ 'ë§ë íë¦¼'ê³¼ 'ì¶”ì„ìƒˆ'ë¥¼ íƒì§€í•˜ê³ , "
        "JSON í˜•ì‹ìœ¼ë¡œë§Œ ë°˜í™˜í•˜ì„¸ìš”. íƒì§€ í›„, íƒì§€ëœ ëª¨ë“  ìš”ì†Œë¥¼ ì œê±°í•œ ì •ì œ í…ìŠ¤íŠ¸ë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”."
        f"íƒì§€ ê¸°ì¤€: ë§ë íë¦¼ ({HESITATION_LIST}), ì¶”ì„ìƒˆ ({FILLER_LIST})."
    )

    try:
        completion = client.chat.completions.create(
            model="gpt-3.5-turbo-1106",
            response_format={"type": "json_object"},
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": full_text}]
        )
        return json.loads(completion.choices[0].message.content)

    except Exception as e:
        print(f"âŒ GPT API í˜¸ì¶œ (ì–¸ì–´ ìŠµê´€) ì‹¤íŒ¨: {e}")
        return {}


# -----------------------------------------------------------------
# 3. WPM, ë¬´ìŒ êµ¬ê°„ ë° ì–¸ì–´ ìŠµê´€ ë¶„ì„ í†µí•© ë¡œì§
# -----------------------------------------------------------------
def analyze_voice_rhythm_and_patterns(stt_result_data: dict) -> dict:
    """WPM, ë¬´ìŒ êµ¬ê°„ íƒì§€ ë° GPT ê¸°ë°˜ ì¶”ì„ìƒˆ/ë§ë íë¦¼ ë¶„ì„ì„ í†µí•© ìˆ˜í–‰í•©ë‹ˆë‹¤."""

    # 2-1. WPM ë° ë¬´ìŒ êµ¬ê°„ ë¶„ì„ 
    words = stt_result_data.get('words', [])
    total_duration = stt_result_data.get('duration_sec', 0.0)
    word_count = len(words)
    wpm = round((word_count / total_duration) * 60) if total_duration > 0 else 0

    pause_events: List[Dict] = []
    all_pause_durations: List[float] = []

    for i in range(len(words) - 1):
        current_word_end = words[i].get('end', 0.0)
        next_word_start = words[i+1].get('start', 0.0)
        gap_duration = next_word_start - current_word_end
        if gap_duration > 0: all_pause_durations.append(gap_duration)
        if gap_duration >= PAUSE_THRESHOLD_SEC:
            pause_events.append({
                "start_sec": round(current_word_end, 2), "end_sec": round(next_word_start, 2),
                "duration": round(gap_duration, 2)
            })

    total_pause_count = len(all_pause_durations)
    avg_pause_duration = round(sum(all_pause_durations) / total_pause_count, 2) if total_pause_count > 0 else 0.0
    long_pause_count = len(pause_events)
    full_text = stt_result_data.get('full_text', '')


    # 2-2. GPT ê¸°ë°˜ ì¶”ì„ìƒˆ/ë§ë íë¦¼ ë¶„ì„ 
    speech_patterns_result = analyze_speech_patterns_with_gpt(full_text)

    # 2-3. ìµœì¢… í†µí•© ê²°ê³¼ êµ¬ì„±
    return {
        "raw_text_for_gpt": full_text,

        "wpm": wpm,
        "pause_events": pause_events,
        "avg_pause_duration": avg_pause_duration,
        "long_pause_count": long_pause_count,

        "hesitation_count": speech_patterns_result.get('hesitation_count', 0),
        "filler_count": speech_patterns_result.get('filler_count', 0),
        "hesitation_list": speech_patterns_result.get('hesitation_list', []),
        "filler_list": speech_patterns_result.get('filler_list', []),
        "text_for_logic_analysis": speech_patterns_result.get('text_for_logic_analysis', full_text),
    }

# -----------------------------------------------------------------
# 4. í†µí•© ë°°ì¹˜ ì²˜ë¦¬ í•¨ìˆ˜ (ë©”ì¸ ë¡œì§)
# -----------------------------------------------------------------
def process_multiple_videos(input_dir: str, output_dir_audio: str, output_dir_json: str, user_id: str) -> None:

    is_firebase_ok = initialize_firebase()

    os.makedirs(output_dir_audio, exist_ok=True)
    os.makedirs(output_dir_json, exist_ok=True)
    video_files = [f for f in os.listdir(input_dir) if f.endswith('.mp4')]

    if not video_files:
        print(f"ê²½ê³ : '{input_dir}'ì—ì„œ ì²˜ë¦¬í•  MP4 ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    print(f"ì´ {len(video_files)}ê°œì˜ ì˜ìƒì„ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‚¬ìš©ì ID: {user_id}")

    for i, video_file in enumerate(video_files):
        print(f"\n--- [{i+1}/{len(video_files)}] {video_file} ì²˜ë¦¬ ì‹œì‘ ---")

        video_path = os.path.join(input_dir, video_file)
        base_name = os.path.splitext(video_file)[0]
        audio_path = os.path.join(output_dir_audio, f"{base_name}.wav")
        full_text_txt_path = os.path.join(output_dir_json, f"{base_name}_fulltext.txt")
        json_path = os.path.join(output_dir_json, f"{base_name}_analysis_data.json")

        # 1. ì˜¤ë””ì˜¤ ì¶”ì¶œ
        if not extract_audio(video_path, audio_path):
             continue

        # 2. Whisper STT ì „ì‚¬
        stt_data = whisper_transcribe(audio_path)

        if stt_data:
            # 3. WPM, ë¬´ìŒ, ì¶”ì„ìƒˆ, ë§ë íë¦¼ ë¶„ì„ í†µí•© ìˆ˜í–‰
            print("  WPM ë° ì–¸ì–´ ìŠµê´€ ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
            voice_analysis_result = analyze_voice_rhythm_and_patterns(stt_data)
            try:
                full_text_content = voice_analysis_result['raw_text_for_gpt']
                with open(full_text_txt_path, 'w', encoding='utf-8') as f:
                    f.write(full_text_content)
                print(f"  âœ… Full Text TXT íŒŒì¼ ì €ì¥ ì™„ë£Œ: {full_text_txt_path}")
            except Exception as e:
                print(f"  âŒ Full Text TXT ì €ì¥ ì‹¤íŒ¨: {e}")
            # 4. ë¡œì»¬ JSON íŒŒì¼ ì €ì¥ (WPM ë¶„ì„ ê²°ê³¼ í¬í•¨)
            final_analysis_data = {
                "stt_raw": stt_data,
                "voice_analysis": voice_analysis_result
            }
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump(final_analysis_data, f, ensure_ascii=False, indent=4)
            print(f"  âœ… ìµœì¢… ë¶„ì„ ìë£Œ JSON ì €ì¥ ì™„ë£Œ: {json_path}")

            # 5. Firebase DBì— WPM ë¶„ì„ ê²°ê³¼ ì—…ë¡œë“œ
            if is_firebase_ok:
                print("  Firebase DBì— ë¶„ì„ ê²°ê³¼ ì—…ë¡œë“œ ì¤‘...")
                upload_to_firebase_analysis(user_id, base_name, voice_analysis_result)

# --- ìµœì¢… ì‹¤í–‰ ---
# ì´ ë¶€ë¶„ì„ ì‹¤í–‰í•˜ëŠ” ì…€ì´ ìœ„ ëª¨ë“  í•¨ìˆ˜ ì •ì˜ ì…€ë³´ë‹¤ ë‚˜ì¤‘ì— ì‹¤í–‰ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
process_multiple_videos(INPUT_VIDEO_DIR, OUTPUT_AUDIO_DIR, OUTPUT_JSON_DIR, USER_ID)
